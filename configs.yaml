defaults:
  batch_size: 64
  model: 'Decoder_BCD'
  dataset: 'er'
  datatype: 'er'
  ckpt_id: 'linear_decoder_bcd'

  # ? Directories
  logdir: "logs"
  user_dir: "/home/jithen"            
  storage_dir: "/home/jithen/scratch"
  data_dir: 'datasets/CLEVR_v1.0/images'      # user_dir/storage_dir/data_dir
  model_params_logdir: "model_params"         # has the form logs/<model_name>/model_params/<id>_00000xxxxx.pickle

  # ? wandb
  offline_wandb: False
  off_wandb: True
  wandb_project: 'structure-learning'
  wandb_entity: 'structurelearning'
  log_batches: 4

  # ? Logging and checkpoint frequencies (steps)
  loss_log_freq: 100
  media_log_freq: 500
  ckpt_save_freq: 5000

  # ? Dataset details
  resolution: 64
  channels: 3

  # ? Slot attention params
  num_slots: 7
  slot_size: 64
  num_iterations: 3

  # ? model architecture sizes
  encoder_out_ch: 64

  # ? hyperparams
  lr: 2e-3
  decay_rate: 0.5
  steps: 10000
  clip: -1

  # misc
  phase: 'train'

  # ? VCN params
  num_nodes: 5
  seed: 10
  data_seed: 2
  num_samples: 200
  no_autoreg_base: False
  sparsity_factor: 1e-3
  gibbs_temp_init: 10
  gibbs_temp: 1000
  theta_mu: 2.0
  theta_sigma: 1.0
  data_type: 'er'
  eval_only: False
  anneal: True
  alpha_mu: 1.0
  alpha_lambd: 10.
  hidden_dims: 64
  factorised: False
  # For synthetic Erdos-Renyi data
  noise_sigma: 0.1
  noise_mu: 0.0
  noise_type: 'isotropic-gaussian'
  proj_dims: 10
  known_ED: False   # if known_ED is true, projection matrix W is used as decoder W
  noise_ED: False   # if known_ED and noise_ED is true, encoder and decoder becomes (W+noise) and (W+noise).T

  # ? Graph VAE
  total_dims: 80

  # ? VAE-VCN
  opti: 'alt' # ['simult', 'alt', 'fast-slow'] for optimizing latent variable params and graph structure params

  # ? DIBS
  h_latent: 5.0
  h_theta: 1.0
  alpha_linear: 0.1
  num_updates: 1000
  n_particles: 20
  # ? TODO has to be changed to 'reparam' later
  grad_estimator: 'reparam'  # ['score', 'reparam']   

  # ? Decoder DIBS
  beta: 1.0
  dibs_lr: 5e-3
  soft_constraint: False
  projection: 'random'
  z_prior: 'sample'   # TODO: 'actual' needs fixing from std to covar so use 'sample for now' | ['actual', 'sample'] 
  algo: 'def'
  interv_type: 'single'

  # ? linear_decoder_bcd
  likelihood: 'linear'
  sem_type: 'linear-gauss'

  # ? BCD Nets
  max_deviation: 0.01
  use_alternative_horseshoe_tau: False
  degree: 1
  n_data: 100
  do_ev_noise: True
  use_flow: False # ! no support for use_flow True
  hidden_size: 128
  num_perm_layers: 2
  print_golem_solution: False # ! no support for True
  factorized: False  # ! no support for True
  subsample: False   # ! no support for True
  fixed_tau: 0.2
  logit_constraint: 10
  s_prior_std: 3.0
  num_outer: 1
  eval_eid: False
  
  
  # ? Decoder BCD: Don't change these default values! Always modify in separate config keys like below
  learn_P: False
  learn_L: True
  learn_noise: True
  L_KL: False
  P_KL: False
  Z_KL: 'joint' # 'joint' or 'conditional'
  tau_scaling: False
  train_loss: 'mse'
  decoder_layers: 'linear'
  l2_reg: False  
  cov_space: False
  kl_nodes: 1
  reg_decoder: False
  pred_last_L: 1
  fix_edges: False
  z_mse: False
  num_steps: 5000
  proj: 'linear'
  obs_Z_KL: False
  use_proxy: False
  across_interv: False
  identity_proj: False
  proj_sparsity: 0.0
  fix_decoder: False
  exp_edges: 1.0
  n_interv_sets: 20
  interv_value: 'uniform'

obs_supervised_dbcd:
  exp_name: 'supervised decoder bcd (observational)'
  num_nodes: 6
  proj_dims: 10
  data_seed: 2
  off_wandb: True
  num_samples: 600
  obs_data: 600
  interv_type: 'single'
  learn_L: True    # 'partial'
  obs_Z_KL: True
  interv_value: 'uniform'

z_supervised_dbcd_obs:
  exp_name: 'Z MSE supervised decoder bcd (observational)'
  num_nodes: 6
  proj_dims: 6
  data_seed: 2
  num_steps: 5000
  off_wandb: False
  num_samples: 2400
  obs_data: 2400
  interv_type: 'single'
  learn_L: True    # 'partial'
  interv_value: 100.0
  z_mse: True

fixed_decoder_supervised_dbcd:
  exp_name: 'fixed decoder supervised decoder bcd (observational)'
  num_nodes: 6
  proj_dims: 6
  data_seed: 2
  num_steps: 5000
  off_wandb: False
  num_samples: 6000
  obs_data: 6000
  interv_type: 'single'
  learn_L: True    # 'partial'
  obs_Z_KL: True
  interv_value: 100.0

decoder_bcd_across_intervs:
  exp_name: 'linear_decoder_bcd_across_intervs'
  num_nodes: 6
  proj_dims:  10
  data_seed: 1
  off_wandb: True
  num_samples: 3600
  obs_data: 300
  interv_type: 'multi'
  learn_L: True    # 'partial'
  across_interv: True
  interv_value: 100.0
  n_interv_sets: 10

decoder_bcd_across_intervs_learn_noise:
  exp_name: 'linear_decoder_bcd_across_intervs_learn_noise'
  num_nodes: 6
  proj_dims: 10
  data_seed: 1
  off_wandb: False
  num_samples: 3600
  obs_data: 300
  interv_type: 'multi'
  learn_L: True    # 'partial'
  across_interv: True
  interv_value: 'uniform'
  learn_noise: True

decoder_bcd_across_intervs_learn_P_noise:
  exp_name: 'decoder_bcd_across_intervs_learn_P_noise'
  num_nodes: 6
  proj_dims: 10
  data_seed: 1
  off_wandb: False
  num_samples: 3900
  obs_data: 300
  interv_type: 'multi'
  learn_L: True    # 'partial'
  learn_noise: True
  learn_P: True
  obs_Z_KL: False
  across_interv: True
  n_interv_sets: 10
  interv_value: 'uniform'

nonlinear_proj_dbcd_intervs:
  exp_name: 'nonlinear_proj_dbcd_intervs'
  num_nodes: 6
  proj_dims:  10
  proj: '2_layer_mlp'
  data_seed: 1
  off_wandb: False
  num_samples: 3600
  obs_data: 300
  interv_type: 'multi'
  learn_L: True    # 'partial'
  across_interv: True
  interv_value: 'uniform'
  n_interv_sets: 10


# ! Learn L linear Decoder BCD

linear_dbcd_obs:
  exp_name: 'Decoder BCD observational learn L (linear projection)'
  exp_edges: 1.0
  num_nodes: 6
  proj_dims: 100
  data_seed: 2
  off_wandb: True
  num_samples: 3000
  obs_data: 3000
  learn_L: True    # 'partial'
  proj: 'linear'
  num_steps: 5000

linear_dbcd_single_interv:
  exp_name: 'Decoder BCD single interventional learn L (linear projection)'
  interv_type: 'single'
  interv_value: 'uniform'
  proj: 'linear'
  num_nodes: 6
  exp_edges: 1.0
  proj_dims: 100
  data_seed: 2
  off_wandb: True
  num_samples: 3500
  obs_data: 500
  learn_L: True    # 'partial'
  n_interv_sets: 20

linear_dbcd_multi_interv:
  exp_name: 'Decoder BCD multi interventional learn L (linear projection)'
  interv_type: 'multi'
  interv_value: 'uniform'
  proj: 'linear'
  num_nodes: 50
  exp_edges: 1.0
  proj_dims: 100
  data_seed: 2
  off_wandb: True
  num_samples: 3500
  obs_data: 500
  learn_L: True    # 'partial'
  n_interv_sets: 20


# ! learn SCM linear Decoder BCD

linear_dbcd_learn_SCM_obs:
  exp_name: 'Decoder BCD observational learn SCM (linear projection)'
  proj: 'linear' 
  interv_type: 'multi'
  interv_value: 'uniform'
  num_nodes: 50
  proj_dims: 100
  data_seed: 2
  off_wandb: True
  num_samples: 3500
  obs_data: 3500
  learn_noise: True
  learn_L: True    # 'partial'
  learn_P: True

linear_dbcd_learn_SCM_single_interv:
  exp_name: 'Decoder BCD single interventional learn SCM (linear projection)'
  proj: 'linear' 
  interv_type: 'single'
  interv_value: 'uniform'
  num_nodes: 6
  proj_dims: 10
  data_seed: 2
  off_wandb: True
  num_samples: 3500
  obs_data: 500
  learn_noise: True
  learn_L: True    # 'partial'
  learn_P: True

linear_dbcd_learn_SCM_multi_interv:
  exp_name: 'Decoder BCD multi interventional learn SCM (linear projection)'
  proj: 'linear' 
  interv_type: 'multi'
  interv_value: 'uniform'
  num_nodes: 6
  proj_dims: 10
  data_seed: 2
  off_wandb: True
  num_samples: 3500
  obs_data: 500
  learn_noise: True
  learn_L: True    # 'partial'
  learn_P: True





nonlinear_proj_dbcd_obs:
  exp_name: 'Decoder BCD observational learn L (nonlinear projection)'
  proj: '2_layer_mlp'
  interv_type: 'multi'
  interv_value: 'uniform'
  num_nodes: 6
  proj_dims: 10
  data_seed: 2
  off_wandb: True
  num_samples: 3900
  obs_data: 3900
  learn_L: True    # 'partial'

nonlinear_proj_dbcd_single_interv:
  exp_name: 'Decoder BCD single interventional learn L (nonlinear projection)'
  proj: '2_layer_mlp'
  interv_type: 'single'
  interv_value: 'uniform'
  num_nodes: 6
  proj_dims: 10
  data_seed: 2
  off_wandb: True
  num_samples: 3500
  obs_data: 500
  learn_L: True    # 'partial'

nonlinear_proj_dbcd_multi_interv:
  exp_name: 'Decoder BCD multi interventional learn L (nonlinear projection)'
  proj: '2_layer_mlp'
  interv_type: 'multi'
  interv_value: 'uniform'
  num_nodes: 6
  proj_dims: 10
  data_seed: 2
  off_wandb: True
  num_samples: 3500
  obs_data: 500
  learn_L: True    # 'partial'


nonlinear_proj_dbcd_learn_SCM:
  exp_name: 'nonlinear_proj_dbcd_learn_P_noise'
  proj: '2_layer_mlp'
  num_nodes: 6
  proj_dims: 10
  data_seed: 3
  off_wandb: True
  num_samples: 3900
  obs_data: 300
  interv_type: 'multi'
  interv_value: 'uniform'
  P_KL: True
  n_interv_sets: 20
  learn_L: True    # 'partial'
  learn_noise: True
  learn_P: True


supervised_linear_proj_vae_bcd:
  exp_name: 'VAE-BCD linear proj (supervised)'
  num_nodes: 6
  proj_dims: 10
  exp_edges: 1.0
  data_seed: 3
  off_wandb: False
  num_samples: 1200
  obs_data: 1200
  interv_type: 'multi'
  interv_value: 'uniform'
  obs_Z_KL: True
  n_interv_sets: 20
  num_steps: 2000


linear_proj_vae_bcd:
  exp_name: 'VAE-BCD linear proj (learn L)'
  proj: 'linear'
  num_nodes: 6
  proj_dims: 10
  exp_edges: 1.0
  data_seed: 3
  off_wandb: False
  num_samples: 6300
  obs_data: 300
  interv_type: 'multi'
  interv_value: 'uniform'
  obs_Z_KL: False
  n_interv_sets: 50
  L_KL: True
  L_KL_coeff: 105.0
  learn_noise: True
  learn_L: True
  num_steps: 10000

linear_proj_vae_bcd_learn_SCM:
  exp_name: 'VAE-BCD linear proj (learn SCM)'
  proj: 'linear'
  num_nodes: 6
  proj_dims: 10
  exp_edges: 1.0
  data_seed: 3
  off_wandb: True
  num_samples: 3300
  obs_data: 300
  interv_type: 'multi'
  interv_value: 'uniform'
  obs_Z_KL: False
  n_interv_sets: 20
  L_KL: True
  L_KL_coeff: 10.0
  learn_P: True

nonlinear_proj_vae_bcd:
  exp_name: 'VAE-BCD nonlinear proj (learn L)'
  proj: '2_layer_mlp'
  num_nodes: 6
  proj_dims: 10
  exp_edges: 1.0
  data_seed: 3
  off_wandb: True
  num_samples: 3300
  obs_data: 300
  interv_type: 'multi'
  interv_value: 'uniform'
  obs_Z_KL: False
  n_interv_sets: 20
  L_KL: True
  L_KL_coeff: 10.0
  learn_noise: True
  learn_L: True

nonlinear_proj_vae_bcd_learn_SCM:
  exp_name: 'VAE-BCD nonlinear proj (learn SCM)'
  proj: '2_layer_mlp'
  num_nodes: 6
  proj_dims: 10
  exp_edges: 1.0
  data_seed: 3
  off_wandb: True
  num_samples: 3300
  obs_data: 300
  interv_type: 'multi'
  interv_value: 'uniform'
  obs_Z_KL: False
  n_interv_sets: 20
  L_KL: False
  L_KL_coeff: 10.0
  learn_P: True
  learn_noise: True
  learn_L: True

linear_dbcd_learn_partial_L:
  exp_name: 'linear_dbcd_learn_partial_L'
  num_nodes: 6
  proj_dims: 10
  learn_L: 'partial'
  num_samples: 1800
  obs_data: 1800
  pred_last_L: 1
  data_seed: 2
  off_wandb: True
  interv_value: 'uniform'

linear_dbcd_learn_partial_L_across_intervs:
  exp_name: 'linear_dbcd_learn_partial_L_across_intervs'
  num_nodes: 6
  proj_dims: 10
  learn_L: 'partial'
  num_samples: 1800
  obs_data: 900
  pred_last_L: 1
  data_seed: 2
  off_wandb: True
  interv_value: 100.0
  interv_type: 'single'



conv_decoder_bcd:
  exp_name: 'Batched Conv. Decoder BCD'
  num_nodes: 6
  exp_edges: 1.0
  data_seed: 2
  num_samples: 3300
  obs_data: 300
  interv_type: 'multi'
  interv_value: 'uniform'
  noise_sigma: 1.5
  dataset: 'chemdata'
  generate: True
  off_wandb: True
  n_interv_sets: 20
  batches: 10
  num_steps: 300
  
conv_decoder_bcd_learn_SCM:
  exp_name: 'Batched Conv. Decoder BCD (learn SCM)'
  num_nodes: 6
  exp_edges: 1.0
  data_seed: 3
  num_samples: 3300
  obs_data: 300
  interv_type: 'multi'
  interv_value: 'uniform'
  noise_sigma: 1.5
  dataset: 'chemdata'
  generate: True
  n_interv_sets: 20
  batches: 50
  num_steps: 2000
  off_wandb: False
  learn_P: True

graphvae:
  exp_name: 'Graph VAE (baseline)' 
  model: GraphVAE
  num_nodes: 6
  exp_edges: 1.0
  data_seed: 2
  num_samples: 3300
  obs_data: 300
  interv_type: 'multi'
  interv_value: 'uniform'
  noise_sigma: 1.5
  generate: True
  n_interv_sets: 20
  batches: 10
  num_steps: 300
  total_dims: 120
  dataset: 'chemdata'
  lr: 1e-3
  off_wandb: False


controlled_supervision:
  exp_name: 'controlled supervision decoder bcd'
  num_nodes: 6
  proj_dims: 6
  data_seed: 2
  num_steps: 1000
  off_wandb: True
  num_samples: 2400
  obs_data: 2100
  interv_type: 'single'
  decoder_layers: 'linear'
  learn_noise: False
  
  learn_L: True    # 'partial'
  obs_Z_KL: False
  use_proxy: True
  edge_noise: 0.1
  pred_last_L: 1

  l2_reg: False
  tau_scaling: False
  kl_nodes: 1
  interv_value: 100.0

sparse_dbcd:
  exp_name: 'sparse projection decoder bcd'
  num_nodes: 6
  proj_dims: 6
  exp_edges: 1.0
  data_seed: 2
  num_steps: 5000
  off_wandb: False
  num_samples: 3000
  obs_data: 3000
  interv_type: 'single'
  decoder_layers: 'linear'
  learn_L: True    # 'partial'
  pred_last_L: 1

  l2_reg: False
  tau_scaling: False
  cov_space: False
  fix_decoder: False
  kl_nodes: 1
  across_interv: False
  reg_decoder: False
  interv_value: 100.0
  proj_sparsity: 1.0
  identity_proj: True



linear_bcd:
  exp_name: 'linear_bcd'
  ckpt_id: 'linear_bcd'
  model: 'BCD'
  num_nodes: 6
  proj_dims: 10
  exp_edges: 1.0
  likelihood: 'linear'
  sem_type: 'linear-gauss'
  lr: 1e-3
  data_seed: 0
  num_steps: 20000
  off_wandb: True
  num_samples: 100
  L_KL: False
  # P_KL: True


# ! Only decoder
decoder:
  exp_name: 'only_decoder'
  ckpt_id: 'only_decoder'
  model: 'Decoder_BCD'
  num_nodes: 6
  proj_dims: 10
  exp_edges: 1.0
  likelihood: 'linear'
  sem_type: 'linear-gauss'
  lr: 1e-3
  data_seed: 0
  num_steps: 5000
  off_wandb: True
  num_samples: 200
  obs_data: 200
  train_loss: 'LL'
  decoder_layers: 'linear'
  learn_noise: False
  learn_L: False
  l2_reg: False
  z_sample: 'sample'
  L_KL: False
  P_KL: False
  Z_KL: False


# ! Dibs configs

# ? 9. Decoder_DIBS
# ! python main.py --config defaults decoder_dibs_er experimental
decoder_dibs_er:
  ckpt_id: 'decoder_dibs_er'
  model: 'Decoder_DIBS'
  proj: 'linear'
  linear_decoder: False
  # ? TODO has to be changed to 'reparam' later
  grad_estimator: 'score' # ['score', 'reparam']
  z_prior: 'sample'   # TODO: 'actual' needs fixing from std to covar so use 'sample for now' | ['actual', 'sample'] 
  algo: 'def'
  num_nodes: 4
  lr: 8e-4

  steps: 1000
  num_updates: 500
  proj_dims: 10
  data_seed: 1
  exp_edges: 0.5
  num_samples: 200
  obs_data: 200
  projection: 'random'
  likelihood: 'nonlinear'  # ['bge', 'nonlinear']
  datagen: 'linear'

# ? DIBS
linear_dibs:
  exp_name: 'linear_joint_dibs'
  ckpt_id: 'train_dibs_er'
  likelihood: 'linear'  # ['bge', 'nonlinear', 'linear']
  datagen: 'linear'
  model: 'DIBS'
  data_seed: 2
  num_nodes: 5
  num_updates: 1000
  num_samples: 500
  obs_data: 300
  across_interv: True
  interv_type: 'single'  # ['single', 'multi']
  exp_edges: 0.5
  grad_estimator: 'reparam'
  off_wandb: False

nonlinear_dibs:
  exp_name: 'nonlinear_joint_dibs'
  ckpt_id: 'train_dibs_er'
  likelihood: 'nonlinear'  # ['bge', 'nonlinear', 'linear']
  datagen: 'nonlinear'
  model: 'DIBS'
  data_seed: 1
  num_nodes: 50
  num_updates: 5000
  num_samples: 200
  obs_data: 100
  across_interv: True
  interv_type: 'single'  # ['single', 'multi']
  exp_edges: 1.0
  grad_estimator: 'reparam'
  off_wandb: True


# ? Linear Decoder DIBS (Joint)
linear_decoder_dibs:
  exp_name: 'linear_decoder_dibs'
  ckpt_id: 'joint_linear_decoder_dibs_er'
  likelihood: 'linear'  # ['bge', 'nonlinear', 'linear']
  datagen: 'linear'
  model: 'Decoder_JointDiBS'
  linear_decoder: False
  num_nodes: 5
  proj_dims: 10
  exp_edges: 0.5
  lr: 8e-4

  steps: 14000
  num_updates: 4000
  data_seed: 12
  num_samples: 500
  obs_data: 300
  off_wandb: True
  clamp: False 
  reinit: False # reinit target and model every iter
  interv_type: 'single'  # ['single', 'multi']
  grad_estimator: 'reparam'
  across_interv: True
  supervised: True
  topsort: False

  # ! num_nodes: 4, 6, 8, 10, 20
  # ! num_updates: 1k, 3k, 3k, 5k, 5k
  # ! proj_dims: 10, 10, 20, 20, 50

  # ! clamp: T, F
  # ! reinit: T, F
  # ! obs_data :
    # ! 100 {50, 100, 200} , 
    #  ! 200 {100, 200, 300}, 
    #  ! 300 {200, 300, 400}

# ? Nonlinear Decoder DIBS (Joint)
nonlinear_decoder_dibs:
  exp_name: 'nonlinear_decoder_dibs'
  ckpt_id: 'joint_nonlinear_decoder_dibs_er'
  likelihood: 'nonlinear'  # ['bge', 'nonlinear', 'linear']
  datagen: 'nonlinear'
  model: 'Decoder_JointDiBS'
  linear_decoder: False
  exp_edges: 0.5
  supervised: True
  lr: 8e-4
  proj_dims: 10

  steps: 1100
  num_nodes: 4
  num_updates: 1000
  data_seed: 1
  num_samples: 100
  obs_data: 100
  across_interv: False
  off_wandb: True
  clamp: False
  reinit: False # reinit target and model every iter
  interv_type: 'single'  # ['single', 'multi']
  grad_estimator: 'reparam'


# ? 8. VAE_DIBS
# ! python main.py --config defaults experimental linear_vaedibs_er
linear_vaedibs_er:
  ckpt_id: 'linear_vaedibs_er'
  model: 'VAE_DIBS'
  dataset: 'er'
  datatype: 'er'
  data_seed: 12
  lr: 9e-4
  num_nodes: 4
  exp_edges: 0.7
  proj: 'linear'
  known_ED: (True,True)
  steps: 1000      # updates for VAE part of VAE DIBS
  num_updates: 500    # updates for DIBS
  proj_dims: 10
  num_samples: 3000
  soft_constraint: False



experimental:
  off_wandb: True

mini:
  loss_log_freq: 5
  media_log_freq: 5
  steps: 100

mila:
  offline_wandb: False


# # ? 1. Vanilla Slot attention
# train_clevr_sa_img:
#   ckpt_id: 'train_clevr_sa_img'
#   model: 'SlotAttention_img'
#   dataset: 'clevr'
#   steps: 100000

# # ? 2a Vanilla VCN (numerical)
# # ! python main.py --config defaults experimental train_vcn
# train_vcn:
#   ckpt_id: 'train_vcn' 
#   dataset: 'er'
#   datatype: 'er'
#   batch_size: 1000
#   seed: 10
#   data_seed: 11
#   lr: 1e-2
#   steps: 30000
#   model: 'VCN'
#   num_nodes: 4
#   opti: 'default'
#   num_samples: 500
#   exp_edges: 0.7

# # ? 3. Image VCN
# train_clevr_vcn_img:
#   ckpt_id: 'train_clevr_vcn_img' 
#   model: 'VCN_img'
#   batch_size: 64
#   lr: 8e-4
#   steps: 30000
#   dataset: 'clevr'
#   datatype: 'image'
#   loss_log_freq: 50
#   media_log_freq: 100
#   ckpt_save_freq: 1000
#   chan_per_node: 6
#   num_nodes: 3

# # ? 4. Slot-Image VCN
# train_clevr_slot1d_vcn_img:
#   ckpt_id: 'train_clevr_slot1d_vcn_img' 
#   model: 'Slot_VCN_img'
#   batch_size: 64
#   lr: 1e-3
#   steps: 20 
#   dataset: 'clevr'
#   loss_log_freq: 20
#   media_log_freq: 50
#   ckpt_save_freq: 1000
#   hidden_dims: 64
#   slot_size: 64
#   num_slots: 3
#   num_nodes: 3
#   slot_space: '1d' # can be 1d or 2d


  

# # ? 6a VAE_VCN: linear projection to higher dims (numerical)
# train_vaevcn_linearproj:
#   ckpt_id: 'train_vaevcn_linearproj' 
#   dataset: 'er'
#   datatype: 'er'
#   seed: 10
#   data_seed: 1
#   lr: 1e-3
#   steps: 30000
#   model: 'VAEVCN'
#   num_nodes: 3
#   proj: 'linear'

# # ? 6b VAE_VCN: linear projection to higher dims (numerical) with known decoder == projection matrix
# train_vaevcn_linearproj_knownED:
#   ckpt_id: 'train_vaevcn_linearproj_knownED'
#   seed: 10
#   data_seed: 1
#   lr: 1e-3
#   steps: 30000
#   model: 'VAEVCN'
#   num_nodes: 3
#   proj: 'linear'
#   known_ED: True

# # ? 6c Vanilla VCN: nonlinear projection to higher dims (numerical)
# train_vcn_nonlinearproj:
#   ckpt_id: 'train_vcn_nonlinearproj' 
#   seed: 10
#   data_seed: 1
#   lr: 1e-2
#   steps: 30000
#   model: 'VCN'
#   datatype: 'er'
#   loss_log_freq: 100
#   media_log_freq: 500
#   ckpt_save_freq: 5000
#   num_nodes: 3
#   proj: 'nonlinear'

