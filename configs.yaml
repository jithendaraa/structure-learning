defaults:
  batch_size: 16
  model: 'SlotAttention_img'
  dataset: 'clevr'
  ckpt_id: 'train_clevr_sa_img'

  # ? Directories
  logdir: "logs"
  user_dir: "/home/jithen"            
  storage_dir: "/home/jithen/scratch"
  data_dir: 'datasets/CLEVR_v1.0/images'      # user_dir/storage_dir/data_dir
  model_params_logdir: "model_params"         # has the form logs/<model_name>/model_params/<id>_00000xxxxx.pickle

  # ? wandb
  offline_wandb: True
  off_wandb: False
  wandb_project: 'structure-learning'
  wandb_entity: 'jithendaraa'
  log_batches: 4

  # ? Logging and checkpoint frequencies (steps)
  loss_log_freq: 100
  media_log_freq: 500
  ckpt_save_freq: 5000

  # ? Dataset details
  resolution: 64
  channels: 3

  # ? Slot attention params
  num_slots: 7
  slot_size: 64
  num_iterations: 3

  # ? model architecture sizes
  encoder_out_ch: 64

  # ? hyperparams
  lr: 4e-4
  decay_rate: 0.5
  # decay_steps: 
  steps: 110000
  clip: -1

  # misc
  phase: 'train'

  # ? VCN params
  num_nodes: 2
  seed: 10
  data_seed: 2
  num_samples: 200
  no_autoreg_base: False
  sparsity_factor: 1e-3
  gibbs_temp_init: 10
  gibbs_temp: 1000
  theta_mu: 2.0
  theta_sigma: 1.0
  data_type: 'er'
  exp_edges: 1.0
  eval_only: False
  anneal: True
  alpha_mu: 1.0
  alpha_lambd: 10.
  hidden_dims: 64
  factorised: False
  # For synthetic Erdos-Renyi data
  noise_sigma: 1.0
  noise_mu: 0.0
  noise_type: 'isotropic-gaussian'
  proj: 'no'        # [False, 'linear', 'nonlinear']
  proj_dims: 10
  known_ED: False   # if known_ED is true, projection matrix W is used as decoder W
  noise_ED: False   # if known_ED and noise_ED is true, encoder and decoder becomes (W+noise) and (W+noise).T

  # ? Graph VAE
  M: 80 # M - total dims per node
  N: 5 # num nodes
  dims_per_node: 16 

  # ? VAE-VCN
  opti: 'alt' # ['simult', 'alt'] for optimizing latent variable params and graph structure params

  # ? DIBS
  h_latent: 5.0
  alpha_linear: 0.1
  num_updates: 1000
  n_particles: 20

  # ? VAE DIBS
  soft_constraint: True

# ? 1. Vanilla Slot attention
train_clevr_sa_img:
  ckpt_id: 'train_clevr_sa_img'
  model: 'SlotAttention_img'
  dataset: 'clevr'
  steps: 100000

# ? 2a Vanilla VCN (numerical)
# ! python main.py --config defaults experimental train_vcn
train_vcn:
  ckpt_id: 'train_vcn' 
  dataset: 'er'
  datatype: 'er'
  batch_size: 1000
  seed: 10
  data_seed: 10
  lr: 1e-2
  steps: 30000
  model: 'VCN'
  num_nodes: 4
  opti: 'default'
  num_samples: 500
  exp_edges: 0.7


# ? 3. Image VCN
train_clevr_vcn_img:
  ckpt_id: 'train_clevr_vcn_img' 
  model: 'VCN_img'
  batch_size: 64
  lr: 8e-4
  steps: 30000
  dataset: 'clevr'
  datatype: 'image'
  loss_log_freq: 50
  media_log_freq: 100
  ckpt_save_freq: 1000
  chan_per_node: 6
  num_nodes: 3

# ? 4. Slot-Image VCN
train_clevr_slot1d_vcn_img:
  ckpt_id: 'train_clevr_slot1d_vcn_img' 
  model: 'Slot_VCN_img'
  batch_size: 64
  lr: 1e-3
  steps: 20 
  dataset: 'clevr'
  loss_log_freq: 20
  media_log_freq: 50
  ckpt_save_freq: 1000
  hidden_dims: 64
  slot_size: 64
  num_slots: 3
  num_nodes: 3
  slot_space: '1d' # can be 1d or 2d

# ? 5. Graph VAE
train_graph_vae:
  ckpt_id: 'train_graph_vae' 
  model: 'GraphVAE'
  batch_size: 64
  lr: 1e-3
  steps: 2000
  dataset: 'mnist'
  loss_log_freq: 10
  media_log_freq: 10
  ckpt_save_freq: 1000
  N: 5
  M: 80
  dims_per_node: 16 # N': dims per node
  

# ? 6a VAE_VCN: linear projection to higher dims (numerical)
train_vaevcn_linearproj:
  ckpt_id: 'train_vaevcn_linearproj' 
  dataset: 'er'
  datatype: 'er'
  batch_size: 1000
  seed: 10
  data_seed: 1
  lr: 1e-3
  steps: 30000
  model: 'VAEVCN'
  num_nodes: 3
  proj: 'linear'

# ? 6b VAE_VCN: linear projection to higher dims (numerical) with known decoder == projection matrix
train_vaevcn_linearproj_knownED:
  ckpt_id: 'train_vaevcn_linearproj_knownED'
  dataset: 'er'
  datatype: 'er'
  batch_size: 1000
  seed: 10
  data_seed: 1
  lr: 1e-3
  steps: 30000
  model: 'VAEVCN'
  num_nodes: 3
  proj: 'linear'
  known_ED: True

# ? 6c Vanilla VCN: nonlinear projection to higher dims (numerical)
train_vcn_nonlinearproj:
  ckpt_id: 'train_vcn_nonlinearproj' 
  dataset: 'er'
  batch_size: 1000
  seed: 10
  data_seed: 1
  lr: 1e-2
  steps: 30000
  model: 'VCN'
  datatype: 'er'
  loss_log_freq: 100
  media_log_freq: 500
  ckpt_save_freq: 5000
  num_nodes: 3
  proj: 'nonlinear'

# ? 7. DIBS
# ! python main.py --config defaults experimental train_dibs_er
train_dibs_er:
  ckpt_id: 'train_dibs_er'
  model: 'DIBS'
  dataset: 'er'
  datatype: 'er'
  batch_size: 1000
  seed: 10
  data_seed: 10
  lr: 1e-2
  num_nodes: 4
  exp_edges: 0.7
  proj: 'no'
  num_updates: 500
  num_samples: 500
  grad_estimator: 'reparam' # ['score', 'reparam']
  n_particles: 5


# ? 8. VAE_DIBS
# ! python main.py --config defaults experimental linear_vaedibs_er
linear_vaedibs_er:
  ckpt_id: 'linear_vaedibs_er'
  model: 'VAE_DIBS'
  dataset: 'er'
  datatype: 'er'
  batch_size: 1000
  data_seed: 12
  lr: 9e-4
  num_nodes: 4
  exp_edges: 0.7
  proj: 'linear'
  known_ED: (True,True)
  steps: 1000      # updates for VAE part of VAE DIBS
  num_updates: 500    # updates for DIBS
  proj_dims: 10
  num_samples: 3000
  soft_constraint: False

# ? 9. Linear Decoder_DIBS
# ! python main.py --config defaults experimental linear_decoderdibs_er
linear_decoderdibs_er:
  ckpt_id: 'linear_decoderdibs_er'
  model: 'Decoder_DIBS'
  dataset: 'er'
  datatype: 'er'
  batch_size: 1000
  data_seed: 10
  num_nodes: 4
  exp_edges: 0.7
  proj: 'linear'
  steps: 200      # updates for VAE part
  num_updates: 1    # updates for DIBS
  proj_dims: 10
  soft_constraint: False
  known_ED: False
  num_samples: 3000
  lr: 8e-4
  n_particles: 10

experimental:
  off_wandb: True

mini:
  loss_log_freq: 5
  media_log_freq: 5
  steps: 100

mila:
  offline_wandb: False

