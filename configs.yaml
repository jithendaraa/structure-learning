defaults:
  batch_size: 64
  model: 'SlotAttention_img'
  dataset: 'er'
  datatype: 'er'
  ckpt_id: 'train_clevr_sa_img'

  # ? Directories
  logdir: "logs"
  user_dir: "/home/jithen"            
  storage_dir: "/home/jithen/scratch"
  data_dir: 'datasets/CLEVR_v1.0/images'      # user_dir/storage_dir/data_dir
  model_params_logdir: "model_params"         # has the form logs/<model_name>/model_params/<id>_00000xxxxx.pickle

  # ? wandb
  offline_wandb: False
  off_wandb: True
  wandb_project: 'structure-learning'
  wandb_entity: 'structurelearning'
  log_batches: 4

  # ? Logging and checkpoint frequencies (steps)
  loss_log_freq: 100
  media_log_freq: 500
  ckpt_save_freq: 5000

  # ? Dataset details
  resolution: 64
  channels: 3

  # ? Slot attention params
  num_slots: 7
  slot_size: 64
  num_iterations: 3

  # ? model architecture sizes
  encoder_out_ch: 64

  # ? hyperparams
  lr: 5e-3
  decay_rate: 0.5
  steps: 10000
  clip: -1

  # misc
  phase: 'train'

  # ? VCN params
  num_nodes: 2
  seed: 10
  data_seed: 2
  num_samples: 200
  no_autoreg_base: False
  sparsity_factor: 1e-3
  gibbs_temp_init: 10
  gibbs_temp: 1000
  theta_mu: 2.0
  theta_sigma: 1.0
  data_type: 'er'
  eval_only: False
  anneal: True
  alpha_mu: 1.0
  alpha_lambd: 10.
  hidden_dims: 64
  factorised: False
  # For synthetic Erdos-Renyi data
  noise_sigma: 0.1
  noise_mu: 0.0
  noise_type: 'isotropic-gaussian'
  proj_dims: 10
  known_ED: False   # if known_ED is true, projection matrix W is used as decoder W
  noise_ED: False   # if known_ED and noise_ED is true, encoder and decoder becomes (W+noise) and (W+noise).T

  # ? Graph VAE
  M: 80 # M - total dims per node
  N: 5 # num nodes
  dims_per_node: 16 

  # ? VAE-VCN
  opti: 'alt' # ['simult', 'alt', 'fast-slow'] for optimizing latent variable params and graph structure params

  # ? DIBS
  h_latent: 5.0
  h_theta: 1.0
  alpha_linear: 0.1
  num_updates: 1000
  n_particles: 20
  # ? TODO has to be changed to 'reparam' later
  grad_estimator: 'reparam'  # ['score', 'reparam']   
  exp_edges: 0.5

  # ? Decoder DIBS
  beta: 1.0
  dibs_lr: 5e-3
  soft_constraint: False
  projection: 'random'
  proj: 'linear'
  z_prior: 'sample'   # TODO: 'actual' needs fixing from std to covar so use 'sample for now' | ['actual', 'sample'] 
  algo: 'def'
  interv_type: 'single'

  # ? BCD Nets
  max_deviation: 0.01
  use_alternative_horseshoe_tau: False
  degree: 1
  n_data: 100
  do_ev_noise: True
  use_flow: False # ! no support for use_flow True
  hidden_size: 128
  num_perm_layers: 2
  print_golem_solution: False # ! no support for True
  factorized: False  # ! no support for True
  subsample: False   # ! no support for True
  fixed_tau: 0.2
  logit_constraint: 10
  s_prior_std: 3.0
  num_outer: 1
  eval_eid: False

# ? DIBS
linear_dibs:
  exp_name: 'linear_joint_dibs'
  ckpt_id: 'train_dibs_er'
  likelihood: 'linear'  # ['bge', 'nonlinear', 'linear']
  datagen: 'linear'
  model: 'DIBS'
  data_seed: 2
  num_nodes: 4
  num_updates: 1000
  num_samples: 200
  obs_data: 100
  across_interv: True
  interv_type: 'single'  # ['single', 'multi']
  exp_edges: 0.5
  grad_estimator: 'reparam'
  off_wandb: True

nonlinear_dibs:
  exp_name: 'nonlinear_joint_dibs'
  ckpt_id: 'train_dibs_er'
  likelihood: 'nonlinear'  # ['bge', 'nonlinear', 'linear']
  datagen: 'nonlinear'
  model: 'DIBS'
  data_seed: 1
  num_nodes: 50
  num_updates: 5000
  num_samples: 200
  obs_data: 100
  across_interv: True
  interv_type: 'single'  # ['single', 'multi']
  exp_edges: 1.0
  grad_estimator: 'reparam'
  off_wandb: True


# ? Linear Decoder DIBS (Joint)
linear_decoder_dibs:
  exp_name: 'linear_decoder_dibs'
  ckpt_id: 'joint_linear_decoder_dibs_er'
  likelihood: 'linear'  # ['bge', 'nonlinear', 'linear']
  datagen: 'linear'
  model: 'Decoder_JointDiBS'
  linear_decoder: False
  num_nodes: 4
  proj_dims: 10
  exp_edges: 0.5
  lr: 8e-4

  steps: 11000
  num_updates: 1000
  data_seed: 1
  num_samples: 300
  obs_data: 100
  off_wandb: True
  clamp: False 
  reinit: False # reinit target and model every iter
  interv_type: 'single'  # ['single', 'multi']
  grad_estimator: 'reparam'
  across_interv: True
  supervised: True
  topsort: False

  # ! num_nodes: 4, 6, 8, 10, 20
  # ! num_updates: 1k, 3k, 3k, 5k, 5k
  # ! proj_dims: 10, 10, 20, 20, 50

  # ! clamp: T, F
  # ! reinit: T, F
  # ! obs_data :
    # ! 100 {50, 100, 200} , 
    #  ! 200 {100, 200, 300}, 
    #  ! 300 {200, 300, 400}

# ? Nonlinear Decoder DIBS (Joint)
nonlinear_decoder_dibs:
  exp_name: 'nonlinear_decoder_dibs'
  ckpt_id: 'joint_nonlinear_decoder_dibs_er'
  likelihood: 'nonlinear'  # ['bge', 'nonlinear', 'linear']
  datagen: 'nonlinear'
  model: 'Decoder_JointDiBS'
  linear_decoder: False
  exp_edges: 0.5
  supervised: True
  lr: 8e-4
  proj_dims: 10

  steps: 1100
  num_nodes: 4
  num_updates: 1000
  data_seed: 1
  num_samples: 100
  obs_data: 100
  across_interv: False
  off_wandb: True
  clamp: False
  reinit: False # reinit target and model every iter
  interv_type: 'single'  # ['single', 'multi']
  grad_estimator: 'reparam'


linear_bcd:
  exp_name: 'linear_bcd'
  ckpt_id: 'linear_bcd'
  model: 'BCD'
  num_nodes: 4
  proj_dims: 10
  exp_edges: 1.0
  likelihood: 'linear'
  sem_type: 'linear-gauss'
  lr: 1e-3
  data_seed: 0
  num_steps: 3000
  off_wandb: True
  num_samples: 100


linear_decoder_bcd:
  exp_name: 'linear_decoder_bcd'
  ckpt_id: 'linear_decoder_bcd'
  model: 'Decoder_BCD'
  num_nodes: 6
  proj_dims: 10
  exp_edges: 1.0
  likelihood: 'linear'
  sem_type: 'linear-gauss'
  lr: 1e-3
  data_seed: 4
  num_steps: 3000
  off_wandb: True
  num_samples: 100
  obs_data: 100
  learn_noise: False


# ? 9. Decoder_DIBS
# ! python main.py --config defaults decoder_dibs_er experimental
decoder_dibs_er:
  ckpt_id: 'decoder_dibs_er'
  model: 'Decoder_DIBS'
  proj: 'linear'
  linear_decoder: False
  # ? TODO has to be changed to 'reparam' later
  grad_estimator: 'score' # ['score', 'reparam']
  z_prior: 'sample'   # TODO: 'actual' needs fixing from std to covar so use 'sample for now' | ['actual', 'sample'] 
  algo: 'def'
  num_nodes: 4
  lr: 8e-4

  steps: 1000
  num_updates: 500
  proj_dims: 10
  data_seed: 1
  exp_edges: 0.5
  num_samples: 200
  obs_data: 200
  projection: 'random'
  likelihood: 'nonlinear'  # ['bge', 'nonlinear']
  datagen: 'linear'


# ? 8. VAE_DIBS
# ! python main.py --config defaults experimental linear_vaedibs_er
linear_vaedibs_er:
  ckpt_id: 'linear_vaedibs_er'
  model: 'VAE_DIBS'
  dataset: 'er'
  datatype: 'er'
  data_seed: 12
  lr: 9e-4
  num_nodes: 4
  exp_edges: 0.7
  proj: 'linear'
  known_ED: (True,True)
  steps: 1000      # updates for VAE part of VAE DIBS
  num_updates: 500    # updates for DIBS
  proj_dims: 10
  num_samples: 3000
  soft_constraint: False



experimental:
  off_wandb: True

mini:
  loss_log_freq: 5
  media_log_freq: 5
  steps: 100

mila:
  offline_wandb: False


# ? 1. Vanilla Slot attention
train_clevr_sa_img:
  ckpt_id: 'train_clevr_sa_img'
  model: 'SlotAttention_img'
  dataset: 'clevr'
  steps: 100000

# ? 2a Vanilla VCN (numerical)
# ! python main.py --config defaults experimental train_vcn
train_vcn:
  ckpt_id: 'train_vcn' 
  dataset: 'er'
  datatype: 'er'
  batch_size: 1000
  seed: 10
  data_seed: 11
  lr: 1e-2
  steps: 30000
  model: 'VCN'
  num_nodes: 4
  opti: 'default'
  num_samples: 500
  exp_edges: 0.7

# ? 3. Image VCN
train_clevr_vcn_img:
  ckpt_id: 'train_clevr_vcn_img' 
  model: 'VCN_img'
  batch_size: 64
  lr: 8e-4
  steps: 30000
  dataset: 'clevr'
  datatype: 'image'
  loss_log_freq: 50
  media_log_freq: 100
  ckpt_save_freq: 1000
  chan_per_node: 6
  num_nodes: 3

# ? 4. Slot-Image VCN
train_clevr_slot1d_vcn_img:
  ckpt_id: 'train_clevr_slot1d_vcn_img' 
  model: 'Slot_VCN_img'
  batch_size: 64
  lr: 1e-3
  steps: 20 
  dataset: 'clevr'
  loss_log_freq: 20
  media_log_freq: 50
  ckpt_save_freq: 1000
  hidden_dims: 64
  slot_size: 64
  num_slots: 3
  num_nodes: 3
  slot_space: '1d' # can be 1d or 2d

# ? 5. Graph VAE
train_graph_vae:
  ckpt_id: 'train_graph_vae' 
  model: 'GraphVAE'
  batch_size: 64
  lr: 1e-3
  steps: 2000
  dataset: 'mnist'
  loss_log_freq: 10
  media_log_freq: 10
  ckpt_save_freq: 1000
  N: 5
  M: 80
  dims_per_node: 16 # N': dims per node
  

# ? 6a VAE_VCN: linear projection to higher dims (numerical)
train_vaevcn_linearproj:
  ckpt_id: 'train_vaevcn_linearproj' 
  dataset: 'er'
  datatype: 'er'
  seed: 10
  data_seed: 1
  lr: 1e-3
  steps: 30000
  model: 'VAEVCN'
  num_nodes: 3
  proj: 'linear'

# ? 6b VAE_VCN: linear projection to higher dims (numerical) with known decoder == projection matrix
train_vaevcn_linearproj_knownED:
  ckpt_id: 'train_vaevcn_linearproj_knownED'
  seed: 10
  data_seed: 1
  lr: 1e-3
  steps: 30000
  model: 'VAEVCN'
  num_nodes: 3
  proj: 'linear'
  known_ED: True

# ? 6c Vanilla VCN: nonlinear projection to higher dims (numerical)
train_vcn_nonlinearproj:
  ckpt_id: 'train_vcn_nonlinearproj' 
  seed: 10
  data_seed: 1
  lr: 1e-2
  steps: 30000
  model: 'VCN'
  datatype: 'er'
  loss_log_freq: 100
  media_log_freq: 500
  ckpt_save_freq: 5000
  num_nodes: 3
  proj: 'nonlinear'

